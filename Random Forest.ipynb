{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'SmartScreen' : 'category',\n",
    "    'AVProductStatesIdentifier' : 'float64',\n",
    "    'Census_OEMModelIdentifier' : 'float64',\n",
    "    'Census_FirmwareVersionIdentifier' : 'float64',\n",
    "    'AVProductsInstalled' : 'float64',\n",
    "    'Census_ProcessorModelIdentifier' : 'float64'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed CSV files\n",
    "X_train = pd.read_csv('./X_train.csv', dtype = dtypes)\n",
    "y_train = pd.read_csv('./y_train.csv', dtype = {'HasDetections' : 'int8'})\n",
    "X_test = pd.read_csv('./X_test.csv', dtype = dtypes)\n",
    "y_test = pd.read_csv('./y_test.csv', dtype = {'HasDetections' : 'int8'})\n",
    "# I am not reading validation set because I do not need it for Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a subset of given df which will be used for constructing decision tree in following methods\n",
    "# df : Main dataframe which will be used for sampling\n",
    "# number_of_rows : Number of rows which will be returned\n",
    "# bootstrap_status : Sample with replacement (Bootstrap Sampling) or without replacement\n",
    "# If bootstrap_status is True, sample can choose same row more than once.\n",
    "def create_subset(df, number_of_rows, bootstrap_status):\n",
    "    return df.sample(n = number_of_rows, replace = bootstrap_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates information gain for categoric features (SmartScreen is our only categoric feature)\n",
    "def calculate_information_gain_categoric(X, y, feature_name):\n",
    "    # Calculate information gain before split\n",
    "    total_number_of_zeros = y[y['HasDetections'] == 0].shape[0]\n",
    "    total_number_of_ones = y[y['HasDetections'] == 1].shape[0]\n",
    "    total_length = y.shape[0]\n",
    "    p_zero = total_number_of_zeros / total_length\n",
    "    p_one = total_number_of_ones / total_length\n",
    "    parent_entropy = - p_zero * np.log2(p_zero) - p_one * np.log2(p_one)\n",
    "    # Calculate information gain after split\n",
    "    split_entropy = 0\n",
    "    for unique_value in X[feature_name].unique():\n",
    "        unique_value_entropy = 0\n",
    "        unique_value_length = X[X[feature_name] == unique_value].shape[0]\n",
    "        # Filter y\n",
    "        y_filter = y[y.index.isin(X[X[feature_name] == unique_value].index)]\n",
    "        unique_number_of_zeros = y_filter[y_filter['HasDetections'] == 0].shape[0]\n",
    "        unique_number_of_ones = y_filter[y_filter['HasDetections'] == 1].shape[0]\n",
    "        unique_value_length = X[X[feature_name] == unique_value].shape[0]\n",
    "        p_zero_unique = unique_number_of_zeros / unique_value_length\n",
    "        p_one_unique = unique_number_of_ones / unique_value_length\n",
    "        if(p_zero_unique != 0 and p_one_unique != 0):\n",
    "            unique_value_entropy = - p_zero_unique * np.log2(p_zero_unique) - p_one_unique * np.log2(p_one_unique)\n",
    "        elif(p_zero_unique == 0 and p_one_unique != 0):\n",
    "            unique_value_entropy = - p_one_unique * np.log2(p_one_unique)\n",
    "        elif(p_zero_unique != 0 and p_one_unique == 0):\n",
    "            unique_value_entropy = - p_zero_unique * np.log2(p_zero_unique)\n",
    "        else: # This should never happen\n",
    "            unique_value_entropy = 0\n",
    "        split_entropy = split_entropy + unique_value_entropy * unique_value_length / total_length\n",
    "    information_gain = parent_entropy - split_entropy\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates information gain for numeric features\n",
    "# Minimum value for max_number_of_splits is 2\n",
    "def calculate_information_gain_numeric(X, y, feature_name, max_number_of_splits):\n",
    "    best_threshold_values = None\n",
    "    best_information_gain = -1 # Any negative value is OK\n",
    "    # Calculate information gain before split\n",
    "    total_number_of_zeros = y[y['HasDetections'] == 0].shape[0]\n",
    "    total_number_of_ones = y[y['HasDetections'] == 1].shape[0]\n",
    "    total_length = y.shape[0]\n",
    "    p_zero = total_number_of_zeros / total_length\n",
    "    p_one = total_number_of_ones / total_length\n",
    "    parent_entropy = - p_zero * np.log2(p_zero) - p_one * np.log2(p_one)\n",
    "    # Check every possible splits to find one with highest information gain value\n",
    "    for number_of_splits in range(1, max_number_of_splits): # Number of splits also indicates number of thresholds\n",
    "        if(number_of_splits <= len(X[feature_name].unique())):\n",
    "            sorted_unique_values = X[feature_name].unique()\n",
    "            sorted_unique_values.sort(axis = 0)\n",
    "            list_of_possible_thresholds = [[threshold_value for threshold_value in threshold_values] for threshold_values in itertools.combinations(sorted_unique_values, number_of_splits)]\n",
    "            for threshold_values in list_of_possible_thresholds:\n",
    "                entropy_start = 0\n",
    "                entropy_mid = 0\n",
    "                entropy_end = 0\n",
    "                # I can check the value of start and end at the end, but I cannot do it for mid length\n",
    "                mid_boolean = True\n",
    "                for index, threshold_value in enumerate(threshold_values):\n",
    "                    if(index == 0): # df[value < threshold]\n",
    "                        y_start = y[y.index.isin(X[X[feature_name] < threshold_value].index)]\n",
    "                        start_length = y_start.shape[0] # Length of the start dataframe\n",
    "                        if(start_length != 0):\n",
    "                            number_of_zeros_start = y_start[y_start['HasDetections'] == 0].shape[0]\n",
    "                            number_of_ones_start = y_start[y_start['HasDetections'] == 1].shape[0]\n",
    "                            p_zero_start = number_of_zeros_start / start_length\n",
    "                            p_one_start = number_of_ones_start / start_length\n",
    "                            if(p_zero_start != 0 and p_one_start != 0):\n",
    "                                entropy_start = (- p_zero_start * np.log2(p_zero_start) - p_one_start * np.log2(p_one_start)) * start_length / total_length\n",
    "                            elif(p_zero_start != 0 and p_one_start == 0):\n",
    "                                entropy_start = (- p_zero_start * np.log2(p_zero_start)) * start_length / total_length\n",
    "                            elif(p_zero_start == 0 and p_one_start != 0):\n",
    "                                entropy_start = (- p_one_start * np.log2(p_one_start)) * start_length / total_length\n",
    "                    if(index == len(threshold_values) - 1): # df[threshold < value]\n",
    "                        y_end = y[y.index.isin(X[threshold_value <= X[feature_name]].index)]\n",
    "                        end_length = y_end.shape[0]\n",
    "                        if(end_length != 0):\n",
    "                            number_of_zeros_end = y_end[y_end['HasDetections'] == 0].shape[0]\n",
    "                            number_of_ones_end = y_end[y_end['HasDetections'] == 1].shape[0]\n",
    "                            p_zero_end = number_of_zeros_end / end_length\n",
    "                            p_one_end = number_of_ones_end / end_length\n",
    "                            if(p_zero_end != 0 and p_one_end != 0):\n",
    "                                entropy_end = (- p_zero_end * np.log2(p_zero_end) - p_one_end * np.log2(p_one_end)) * end_length / total_length\n",
    "                            elif(p_zero_end != 0 and p_one_end == 0):\n",
    "                                entropy_end = (- p_zero_end * np.log2(p_zero_end)) * end_length / total_length\n",
    "                            elif(p_zero_end == 0 and p_one_end != 0):\n",
    "                                entropy_end = (- p_one_end * np.log2(p_one_end)) * end_length / total_length\n",
    "                    else: # df[threshold < value < next_threshold]\n",
    "                        y_mid = y[y.index.isin(X[(threshold_value <= X[feature_name]) & (X[feature_name] < threshold_values[index + 1])].index)]\n",
    "                        mid_length = y_mid.shape[0]\n",
    "                        if(mid_length != 0):\n",
    "                            number_of_zeros_mid = y_mid[y_mid['HasDetections'] == 0].shape[0]\n",
    "                            number_of_ones_mid = y_mid[y_mid['HasDetections'] == 1].shape[0]\n",
    "                            p_zero_mid = number_of_zeros_mid / mid_length\n",
    "                            p_one_mid = number_of_ones_mid / mid_length\n",
    "                            if(p_zero_mid != 0 and p_one_mid != 0):\n",
    "                                entropy_mid = entropy_mid + (- p_zero_mid * np.log2(p_zero_mid) - p_one_mid * np.log2(p_one_mid)) * mid_length / total_length\n",
    "                            elif(p_zero_mid != 0 and p_one_end == 0):\n",
    "                                entropy_mid = entropy_mid + (- p_zero_mid * np.log2(p_zero_mid)) * mid_length / total_length\n",
    "                            elif(p_zero_mid == 0 and p_one_end != 0):\n",
    "                                entropy_mid = entropy_mid + (- p_one_mid * np.log2(p_one_mid)) * mid_length / total_length\n",
    "                            # No need for last case\n",
    "                        else:\n",
    "                            # If the length of one of the dataframe\n",
    "                            mid_boolean = False\n",
    "                information_gain = parent_entropy - (entropy_start + entropy_mid + entropy_end)\n",
    "                # Information gain is 0 in wortst case\n",
    "                if(start_length != 0 and end_length != 0 and mid_boolean):\n",
    "                    if(best_information_gain < information_gain):\n",
    "                        best_information_gain = information_gain\n",
    "                        best_threshold_values = threshold_values\n",
    "    # Return only the best infomation gain and its threshold values\n",
    "    return best_information_gain, best_threshold_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.feature_name = None # Feature that is used for that Node (Only usable for non-leave nodes)\n",
    "        self.threshold_values = [] # Threshold values\n",
    "        self.child_nodes = [] # List of pointers to other node objects\n",
    "        self.prediction = None # Prediction value (Only usable for leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a decision tree using recursion and returns the root of that tree\n",
    "# X : Features\n",
    "# y : Label\n",
    "# min_number_of_rows : Minimum number of rows required to make a split\n",
    "# min_information_gain : Minimum information gain value required to make a split\n",
    "# max_number_of_splits : Maximum number of splits can be done for numeric features\n",
    "# max_depth : Maximum depth of each decision tree\n",
    "# current_depth : Indicates the current depth of the tree which will be used for construction\n",
    "numerical = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "def decision_tree(X, y, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth, current_depth = 0):\n",
    "    root = Node() # Empty node\n",
    "    best_feature_name = None\n",
    "    best_threshold_values = None # If this stays None, we know that our feature is categoric\n",
    "    best_information_gain = 0 # Dummy value\n",
    "    if((current_depth < max_depth) and (min_number_of_rows <= X.shape[0])):\n",
    "        # Calculate best feature using information gain\n",
    "        for feature_name in X.columns.values:\n",
    "            # Calculate information gain of each feature\n",
    "            if(X[feature_name].dtype in numerical): # Only numeric features\n",
    "                information_gain, threshold_values = calculate_information_gain_numeric(X, y, feature_name, max_number_of_splits)\n",
    "                if(best_information_gain < information_gain):\n",
    "                    best_feature_name = feature_name\n",
    "                    best_threshold_values = threshold_values\n",
    "                    best_information_gain = information_gain\n",
    "            else: # Only categoric features\n",
    "                information_gain = calculate_information_gain_categoric(X, y, feature_name)\n",
    "                if(best_information_gain < information_gain):\n",
    "                    best_feature_name = feature_name\n",
    "                    best_threshold_values = None\n",
    "                    best_information_gain = information_gain\n",
    "    if(min_information_gain < best_information_gain):\n",
    "        root.feature_name = best_feature_name\n",
    "        if(best_threshold_values == None): # Categoric feature\n",
    "            for unique_value in X[best_feature_name].unique():\n",
    "                X_child = X[X[best_feature_name] == unique_value]\n",
    "                X_child = X_child.drop([best_feature_name], axis = 1)\n",
    "                y_child = y[y.index.isin(X_child.index)]\n",
    "                root.child_nodes.append((decision_tree(X_child, y_child, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth, current_depth + 1), unique_value))\n",
    "        else:\n",
    "            root.threshold_values = best_threshold_values\n",
    "            for index, threshold_value in enumerate(best_threshold_values):\n",
    "                if(index == 0):\n",
    "                    X_child = X[X[best_feature_name] < threshold_value]\n",
    "                    X_child = X_child.drop([best_feature_name], axis = 1)\n",
    "                    y_child = y[y.index.isin(X_child.index)]\n",
    "                    root.child_nodes.append((decision_tree(X_child, y_child, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth, current_depth + 1), (np.nan, threshold_value)))\n",
    "                if(index == len(best_threshold_values) - 1):\n",
    "                    X_child = X[threshold_value <= X[best_feature_name]]\n",
    "                    X_child = X_child.drop([best_feature_name], axis = 1)\n",
    "                    y_child = y[y.index.isin(X_child.index)]\n",
    "                    root.child_nodes.append((decision_tree(X_child, y_child, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth, current_depth + 1), (threshold_value, np.nan)))\n",
    "                else:\n",
    "                    X_child = X[(threshold_value <= X[best_feature_name]) & (X[best_feature_name] < best_threshold_values[index + 1])]\n",
    "                    X_child = X_child.drop([best_feature_name], axis = 1)\n",
    "                    y_child = y[y.index.isin(X_child.index)]\n",
    "                    root.child_nodes.append((decision_tree(X_child, y_child, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth, current_depth + 1), (threshold_value, best_threshold_values[index + 1])))\n",
    "    else:\n",
    "        if(y[y['HasDetections'] == 0].shape[0] < y[y['HasDetections'] == 1].shape[0]):\n",
    "            root.prediction = 1\n",
    "        else:\n",
    "            root.prediction = 0\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(row, tree):\n",
    "    if(tree.feature_name == None): # Leaf node\n",
    "        return tree.prediction\n",
    "    else:\n",
    "        if(len(tree.threshold_values) == 0): # Categoric feature\n",
    "            for child in tree.child_nodes:\n",
    "                if(child[1] == row[tree.feature_name]):\n",
    "                    return make_prediction(row, child[0])\n",
    "            # Unlike numeric features, each categoric value indicates one thing.\n",
    "            return None\n",
    "        else: # Numeric feature\n",
    "            for child in tree.child_nodes:\n",
    "                threshold_values = child[1]\n",
    "                if(np.isnan(threshold_values[0])): # nan < value < threshold)\n",
    "                    if(row[tree.feature_name] < threshold_values[1]):\n",
    "                        return make_prediction(row, child[0])\n",
    "                elif(np.isnan(threshold_values[1])): # threshold < value < nan\n",
    "                    if(threshold_values[0] <= row[tree.feature_name]):\n",
    "                        return make_prediction(row, child[0])\n",
    "                else: # No NaN (threshold_1 < value < threshold_2)\n",
    "                    if((threshold_values[0] <= row[tree.feature_name]) and (row[tree.feature_name] < threshold_values[1])):\n",
    "                        return make_prediction(row, child[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, forest):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    undecidable = 0\n",
    "    for index, row in X.iterrows():\n",
    "        if(index % 20000 == 0):\n",
    "            print('Current index :', index)\n",
    "        real_value = y.loc[index, 'HasDetections'] # Real value from test set\n",
    "        number_of_zeros = 0\n",
    "        number_of_ones = 0\n",
    "        for tree in forest:\n",
    "            # Prediction can takes values 0, 1 and None\n",
    "            prediction = make_prediction(row, tree)\n",
    "            if(prediction == 0):\n",
    "                number_of_zeros = number_of_zeros + 1\n",
    "            elif(prediction == 1):\n",
    "                number_of_ones = number_of_ones + 1\n",
    "        # Majority voting\n",
    "        if(number_of_zeros < number_of_ones): # Predict 1\n",
    "            if(real_value == 1):\n",
    "                true_positive = true_positive + 1\n",
    "            else: # real_value is 0\n",
    "                false_positive = false_positive + 1\n",
    "        elif(number_of_ones < number_of_zeros): # Predict 0\n",
    "            if(real_value == 0):\n",
    "                true_negative = true_negative + 1\n",
    "            else: # real_value is 1\n",
    "                false_negative = false_negative + 1\n",
    "        elif(number_of_ones < number_of_zeros):\n",
    "            undecidable = undecidable + 1\n",
    "    return true_positive, false_positive, true_negative, false_negative, undecidable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct multiple decision trees\n",
    "# number_of_trees : Number of trees in the forest\n",
    "# number_of_rows : Number of rows in subset that will be used for creating decision tree\n",
    "# Rest of the parameters has been explained in previous cells\n",
    "def random_forest(X_train, y_train, number_of_trees, number_of_rows, bootstrap_status = True, min_number_of_rows = 2, min_information_gain = 0.000001, max_number_of_splits = 2, max_depth = float('Inf')):\n",
    "    forest = []\n",
    "    for i in range(0, number_of_trees):\n",
    "        X_tree = create_subset(X_train, number_of_rows, bootstrap_status)\n",
    "        X_tree = X_tree.sort_index()\n",
    "        y_tree = y_train[y_train.index.isin(X_tree.index)]\n",
    "        tree = decision_tree(X_tree, y_tree, min_number_of_rows, min_information_gain, max_number_of_splits, max_depth)\n",
    "        forest.append(tree)\n",
    "        print(str(i + 1) + '-th tree is constructed')\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_predict = timeit.default_timer()\n",
    "number_of_trees = 100\n",
    "number_of_rows = X_train.shape[0] // 20\n",
    "bootstrap_status = True\n",
    "min_number_of_rows = 100\n",
    "min_information_gain = 0.0001\n",
    "max_number_of_split = 2\n",
    "max_depth = 4\n",
    "forest = random_forest(X_train, y_train, number_of_trees, number_of_rows, bootstrap_status, min_number_of_rows, min_information_gain, max_number_of_split, max_depth)\n",
    "stop_predict = timeit.default_timer()\n",
    "print('Runtime of the algorithm is', stop_predict - start_predict, 'seconds')\n",
    "true_positive, false_positive, true_negative, false_negative, undecidable = predict(X_test, y_test, forest)\n",
    "print('Testing Accuracy :', (true_positive + true_negative) / (true_positive + false_positive + true_negative + false_negative))\n",
    "true_positive, false_positive, true_negative, false_negative, undecidable = predict(X_train, y_train, forest)\n",
    "print('Training Accuracy :', (true_positive + true_negative) / (true_positive + false_positive + true_negative + false_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

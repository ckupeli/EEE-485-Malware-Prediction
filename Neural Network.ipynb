{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'SmartScreen' : 'category',\n",
    "    'AVProductStatesIdentifier' : 'float64',\n",
    "    'Census_OEMModelIdentifier' : 'float64',\n",
    "    'Census_FirmwareVersionIdentifier' : 'float64',\n",
    "    'AVProductsInstalled' : 'float64',\n",
    "    'Census_ProcessorModelIdentifier' : 'float64'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed CSV files\n",
    "X_train = pd.read_csv('./X_train.csv', dtype = dtypes)\n",
    "y_train = pd.read_csv('./y_train.csv', dtype = {'HasDetections' : 'int8'})\n",
    "X_test = pd.read_csv('./X_test.csv', dtype = dtypes)\n",
    "y_test = pd.read_csv('./y_test.csv', dtype = {'HasDetections' : 'int8'})\n",
    "X_validation = pd.read_csv('./X_validation.csv', dtype = dtypes)\n",
    "y_validation = pd.read_csv('./y_validation.csv', dtype = {'HasDetections' : 'int8'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df_feature, df_label):\n",
    "    df = df_feature\n",
    "    df['HasDetections'] = df_label\n",
    "    for feature in df.columns:\n",
    "        if(str(df[feature].dtypes) == 'category'):\n",
    "            # Create dummy fields\n",
    "            for unique_value in df[feature].unique():\n",
    "                df[feature + '_' + unique_value] = 0\n",
    "            for index, row in df.iterrows():\n",
    "                if(index % 10000 == 0):\n",
    "                    print('Index : ' + str(index))\n",
    "                df.loc[index, (feature + '_' + row[feature])] = 1\n",
    "            df = df.drop([feature], axis = 1)\n",
    "    return df.drop('HasDetections', axis = 1), df['HasDetections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train, y_train = one_hot_encoding(X_train, y_train)\n",
    "y_train = y_train.to_frame()\n",
    "'''\n",
    "X_train['HasDetections'] = y_train\n",
    "#X_train = pd.get_dummies(X_train, columns=['SmartScreen', 'AVProductStatesIdentifier', 'Census_OEMModelIdentifier', 'Census_FirmwareVersionIdentifier', 'AVProductsInstalled', 'Census_ProcessorModelIdentifier'])\n",
    "X_train = pd.get_dummies(X_train, columns=['SmartScreen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_test, y_test = one_hot_encoding(X_test, y_test)\n",
    "y_test = y_test.to_frame()\n",
    "'''\n",
    "X_test['HasDetections'] = y_test\n",
    "#X_test = pd.get_dummies(X_test, columns=['SmartScreen', 'AVProductStatesIdentifier', 'Census_OEMModelIdentifier', 'Census_FirmwareVersionIdentifier', 'AVProductsInstalled', 'Census_ProcessorModelIdentifier'])\n",
    "X_test = pd.get_dummies(X_test, columns=['SmartScreen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_validation, y_validation = one_hot_encoding(X_validation, y_validation)\n",
    "y_validation = y_validation.to_frame()\n",
    "'''\n",
    "X_validation['HasDetections'] = y_validation\n",
    "#X_validation = pd.get_dummies(X_validation, columns=['SmartScreen', 'AVProductStatesIdentifier', 'Census_OEMModelIdentifier', 'Census_FirmwareVersionIdentifier', 'AVProductsInstalled', 'Census_ProcessorModelIdentifier'])\n",
    "X_validation = pd.get_dummies(X_validation, columns=['SmartScreen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in X_train.columns:\n",
    "    if(feature not in X_validation.columns):\n",
    "        X_validation[feature] = 0\n",
    "\n",
    "for feature in X_validation.columns:\n",
    "    if(feature not in X_train.columns):\n",
    "        X_validation = X_validation.drop(X_validation[X_validation[feature] == 1].index)\n",
    "        X_validation = X_validation.drop([feature], axis = 1)\n",
    "        \n",
    "for feature in X_train.columns:\n",
    "    if(feature not in X_test.columns):\n",
    "        X_test[feature] = 0\n",
    "\n",
    "for feature in X_test.columns:\n",
    "    if(feature not in X_train.columns):\n",
    "        X_test = X_test.drop(X_test[X_test[feature] == 1].index)\n",
    "        X_test = X_test.drop([feature], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['HasDetections'].to_frame()\n",
    "X_train = X_train.drop('HasDetections', axis = 1)\n",
    "\n",
    "y_test = X_test['HasDetections'].to_frame()\n",
    "X_test = X_test.drop('HasDetections', axis = 1)\n",
    "\n",
    "y_validation = X_validation['HasDetections'].to_frame()\n",
    "X_validation = X_validation.drop('HasDetections', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 281761 entries, 0 to 281760\n",
      "Data columns (total 12 columns):\n",
      "AVProductStatesIdentifier           281761 non-null float64\n",
      "Census_OEMModelIdentifier           281761 non-null float64\n",
      "Census_FirmwareVersionIdentifier    281761 non-null float64\n",
      "AVProductsInstalled                 281761 non-null float64\n",
      "Census_ProcessorModelIdentifier     281761 non-null float64\n",
      "SmartScreen_Block                   281761 non-null uint8\n",
      "SmartScreen_ExistsNotSet            281761 non-null uint8\n",
      "SmartScreen_Off                     281761 non-null uint8\n",
      "SmartScreen_On                      281761 non-null uint8\n",
      "SmartScreen_Prompt                  281761 non-null uint8\n",
      "SmartScreen_RequireAdmin            281761 non-null uint8\n",
      "SmartScreen_Warn                    281761 non-null uint8\n",
      "dtypes: float64(5), uint8(7)\n",
      "memory usage: 12.6 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70364 entries, 0 to 70363\n",
      "Data columns (total 12 columns):\n",
      "AVProductStatesIdentifier           70364 non-null float64\n",
      "Census_OEMModelIdentifier           70364 non-null float64\n",
      "Census_FirmwareVersionIdentifier    70364 non-null float64\n",
      "AVProductsInstalled                 70364 non-null float64\n",
      "Census_ProcessorModelIdentifier     70364 non-null float64\n",
      "SmartScreen_Block                   70364 non-null uint8\n",
      "SmartScreen_ExistsNotSet            70364 non-null uint8\n",
      "SmartScreen_Off                     70364 non-null uint8\n",
      "SmartScreen_On                      70364 non-null uint8\n",
      "SmartScreen_Prompt                  70364 non-null uint8\n",
      "SmartScreen_RequireAdmin            70364 non-null uint8\n",
      "SmartScreen_Warn                    70364 non-null uint8\n",
      "dtypes: float64(5), uint8(7)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70117 entries, 0 to 70116\n",
      "Data columns (total 12 columns):\n",
      "AVProductStatesIdentifier           70117 non-null float64\n",
      "Census_OEMModelIdentifier           70117 non-null float64\n",
      "Census_FirmwareVersionIdentifier    70117 non-null float64\n",
      "AVProductsInstalled                 70117 non-null float64\n",
      "Census_ProcessorModelIdentifier     70117 non-null float64\n",
      "SmartScreen_Block                   70117 non-null uint8\n",
      "SmartScreen_ExistsNotSet            70117 non-null uint8\n",
      "SmartScreen_Off                     70117 non-null uint8\n",
      "SmartScreen_On                      70117 non-null uint8\n",
      "SmartScreen_Prompt                  70117 non-null uint8\n",
      "SmartScreen_RequireAdmin            70117 non-null uint8\n",
      "SmartScreen_Warn                    70117 non-null uint8\n",
      "dtypes: float64(5), uint8(7)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_validation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 281761 entries, 0 to 281760\n",
      "Data columns (total 1 columns):\n",
      "HasDetections    281761 non-null int8\n",
      "dtypes: int8(1)\n",
      "memory usage: 275.3 KB\n"
     ]
    }
   ],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70364 entries, 0 to 70363\n",
      "Data columns (total 1 columns):\n",
      "HasDetections    70364 non-null int8\n",
      "dtypes: int8(1)\n",
      "memory usage: 68.8 KB\n"
     ]
    }
   ],
   "source": [
    "y_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70117 entries, 0 to 70116\n",
      "Data columns (total 1 columns):\n",
      "HasDetections    70117 non-null int8\n",
      "dtypes: int8(1)\n",
      "memory usage: 68.6 KB\n"
     ]
    }
   ],
   "source": [
    "y_validation.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Normalize\\nX_train_norm = (X_train - X_train.mean()) / (X_train.max() - X_train.min())\\nX_test_norm = (X_test - X_test.mean()) / (X_test.max() - X_test.min())\\nX_validation_norm = (X_validation - X_validation.mean()) / (X_validation.max() - X_validation.min())'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Normalize\n",
    "X_train_norm = (X_train - X_train.mean()) / (X_train.max() - X_train.min())\n",
    "X_test_norm = (X_test - X_test.mean()) / (X_test.max() - X_test.min())\n",
    "X_validation_norm = (X_validation - X_validation.mean()) / (X_validation.max() - X_validation.min())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, number_of_neurons, name, leakly_relu_parameter = None):\n",
    "        self.number_of_neurons = number_of_neurons\n",
    "        self.name = name # Help us to identify output and input layers\n",
    "        self.input_shape = None\n",
    "        self.weights = None\n",
    "        self.weight_zero = None\n",
    "        self.input_data = None\n",
    "        self.activation_result = None\n",
    "        self.activation_input = None\n",
    "        self.leakly_relu_parameter = leakly_relu_parameter # Only hidden layers uses this value\n",
    "    \n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "        \n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "        \n",
    "    def set_weights(self, weights = None):\n",
    "        if(weights is None):\n",
    "            #self.weights = np.zeros((self.input_shape, self.number_of_neurons))\n",
    "            #self.weights = np.random.uniform(-1, 1, (self.input_shape, self.number_of_neurons))\n",
    "            self.weights = np.random.normal(size = (self.input_shape, self.number_of_neurons), scale = 1e-3)\n",
    "            self.weight_zero = np.zeros((1, self.number_of_neurons))\n",
    "        else:\n",
    "            if((self.input_shape == weights.shape[0]) and (self.number_of_neurons == weights.shape[1])):\n",
    "                self.weights = weights\n",
    "            else:\n",
    "                print('Weight matrix supposed to be ' + str(self.input_shape) + 'x' + str(self.number_of_neurons))\n",
    "    \n",
    "    def get_output_shape(self):\n",
    "        return self.number_of_neurons\n",
    "    \n",
    "    def foward_pass(self, X):\n",
    "        self.input_data = X\n",
    "        if(self.weights is None):\n",
    "            # Input layer has no weight\n",
    "            return X\n",
    "        else:\n",
    "            # X . W + w_zero\n",
    "            if(self.name == 'output'): # Sigmoid (Binary Classification)\n",
    "                self.activation_result = expit(np.dot(X, self.weights) + self.weight_zero)\n",
    "                return self.activation_result\n",
    "            else: # LeaklyReLU (I have encountered dying ReLU during implementation)\n",
    "                # Hidden layers\n",
    "                self.activation_input = np.dot(X, self.weights) + self.weight_zero\n",
    "                self.activation_result = np.maximum(self.leakly_relu_parameter, self.activation_input)\n",
    "                return self.activation_result  \n",
    "    \n",
    "    def backward_pass(self, gradient, learning_rate):\n",
    "        if(self.name == 'output'): # Output layer uses Sigmoid for activation\n",
    "            activation_gradient = self.activation_result * (1 - self.activation_result)\n",
    "            gradient_wrt_activation = np.multiply(activation_gradient, gradient)\n",
    "        else: # Hidden layers use ReLU for activation\n",
    "            activation_gradient = self.activation_input\n",
    "            activation_gradient[activation_gradient > 0] = 1\n",
    "            activation_gradient[activation_gradient <= 0] = self.leakly_relu_parameter\n",
    "            gradient_wrt_activation = np.multiply(activation_gradient, gradient)\n",
    "        # Store previous weight to iterate\n",
    "        prev_weights = self.weights\n",
    "        # Update weight (gradient * input_data)\n",
    "        weights_gradient = np.dot(np.transpose(self.input_data), gradient)\n",
    "        weight_zero_gradient = np.sum(gradient, axis = 0)\n",
    "        self.weights = self.weights - learning_rate * weights_gradient\n",
    "        self.weight_zero = self.weight_zero - learning_rate * weight_zero_gradient\n",
    "        # Return loss with respect to next layer (gradient * activation_gradient * original weight)\n",
    "        return np.dot(gradient_wrt_activation, np.transpose(prev_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.training_error = []\n",
    "        self.validation_error = []\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        if(len(self.layers) != 0): # 1st layer is input layer, which \n",
    "            layer.set_input_shape(self.layers[-1].get_output_shape())\n",
    "            layer.set_weights()\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward_pass(self, X, training = True):\n",
    "        layer_output = X\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer.foward_pass(layer_output)\n",
    "        return layer_output\n",
    "    \n",
    "    def backward_pass(self, loss_gradient, learning_rate):\n",
    "        # Iterate over from the end and not include input layer\n",
    "        for layer in self.layers[:0:-1]:\n",
    "            loss_gradient = layer.backward_pass(loss_gradient, learning_rate)\n",
    "    \n",
    "    def cross_entropy(self, y, p):\n",
    "        p = np.clip(p, 1e-12, 1 - 1e-12) # Get rif of division by 0 error\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    \n",
    "    def gradient_cross_entropy(self, y, p):\n",
    "        p = np.clip(p, 1e-12, 1 - 1e-12) # Get rid of division by 0 error\n",
    "        return - (y / p) + (1 - y) / (1 - p) # Gradient wrt p\n",
    "    \n",
    "    # batch_size : Part of dataset with given size (GD = # of rows in training data, Mini-B GD = n, SGD = 1)\n",
    "    # number_of_epochs : Number of times NN sees the whole training data\n",
    "    def train(self, X_train, y_train, X_validation, y_validation, batch_size, number_of_epochs = 1000, learning_rate = 0.001):\n",
    "        training_error = []\n",
    "        validation_error = []\n",
    "        validation_accuracy = []\n",
    "        for i in range(0, number_of_epochs):\n",
    "            print('Epoch index :', i)\n",
    "            # Create batch with given batch size\n",
    "            batch_error = []\n",
    "            for index in np.arange(0, len(X_train) - 1, batch_size):\n",
    "                '''\n",
    "                if(index % 10000 == 0):\n",
    "                    print('Batch index :', index)'''\n",
    "                start_index = index\n",
    "                end_index = np.minimum(index + batch_size - 1, len(X_train) - 1)\n",
    "                # Shuffle the data prior to selecting values\n",
    "                p = self.forward_pass(X_train.sample(frac = 1, random_state = index).iloc[start_index:end_index])\n",
    "                loss = np.mean(self.cross_entropy(y_train.iloc[start_index:end_index].to_numpy(), p))\n",
    "                batch_error.append(loss)\n",
    "                loss_gradient = self.gradient_cross_entropy(loss, p)\n",
    "                self.backward_pass(loss_gradient, learning_rate)\n",
    "            training_error.append(np.mean(batch_error))\n",
    "            print('Loss (Batch) :', np.mean(batch_error))\n",
    "            # Calculate error using validation set\n",
    "            p_validation = self.forward_pass(X_validation)\n",
    "            loss_validation = np.mean(self.cross_entropy(y_validation.to_numpy(), p_validation))\n",
    "            y_validation_prediction = p_validation\n",
    "            y_validation_prediction[0.5 < y_validation_prediction] = 1\n",
    "            y_validation_prediction[0.5 >= y_validation_prediction] = 0\n",
    "            accuracy = np.sum(y_validation.to_numpy() == y_validation_prediction, axis = 0)[0] / y_validation.shape[0]\n",
    "            validation_error.append(loss_validation)\n",
    "            validation_accuracy.append(accuracy)\n",
    "            print('Loss (Validation) :', loss_validation)\n",
    "            print('Accuracy (Validation) :', accuracy)\n",
    "        return training_error, validation_error, validation_accuracy\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        p = self.forward_pass(X)\n",
    "        loss = np.mean(self.cross_entropy(y.to_numpy(), p))\n",
    "        y_prediction = p\n",
    "        y_prediction[0.5 < y_prediction] = 1\n",
    "        y_prediction[0.5 >= y_prediction] = 0\n",
    "        accuracy = np.sum(y.to_numpy() == y_prediction, axis = 0)[0] / y_validation.shape[0]\n",
    "        print('Loss (Test) :', loss)\n",
    "        print('Accuracy (Test) :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# X_train : Feature set for training\n",
    "# y_train : Label set for training\n",
    "# X_validation : Feature set for validation\n",
    "# y_validation : Label set for validation\n",
    "# neuron_numbers : List of numbers that store number of neurons in each layer\n",
    "# number_of_epochs : Number of times NN sees the whole training data\n",
    "# batch_size : Part of dataset with given size (GD = # of rows in training data, Mini-B GD = n, SGD = 1)\n",
    "def neural_network(number_of_features, neuron_numbers, leakly_relu_parameter):\n",
    "    # Create NN instance\n",
    "    neural_network = NeuralNetwork()\n",
    "    # Create input layer\n",
    "    input_layer = Layer(X_train.shape[1], 'input')\n",
    "    neural_network.add_layer(input_layer)\n",
    "    # Create hidden layers\n",
    "    for index, number_of_neurons in enumerate(neuron_numbers):\n",
    "        layer = Layer(number_of_neurons, 'hidden_' + str(index), leakly_relu_parameter)\n",
    "        neural_network.add_layer(layer)\n",
    "    # Create output layer\n",
    "    output_layer = Layer(1, 'output') # Only one neuron is enough\n",
    "    neural_network.add_layer(output_layer)\n",
    "    \n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch index : 0\n",
      "Loss (Batch) : 0.6931430361118915\n",
      "Loss (Validation) : 0.6931386733271366\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 1\n",
      "Loss (Batch) : 0.6931345011582707\n",
      "Loss (Validation) : 0.6931302043413846\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 2\n",
      "Loss (Batch) : 0.6931259729298807\n",
      "Loss (Validation) : 0.6931217428053259\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 3\n",
      "Loss (Batch) : 0.6931174566543803\n",
      "Loss (Validation) : 0.6931132736749201\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 4\n",
      "Loss (Batch) : 0.693108950893459\n",
      "Loss (Validation) : 0.6931048142882514\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 5\n",
      "Loss (Batch) : 0.6931004431408706\n",
      "Loss (Validation) : 0.6930963792163491\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 6\n",
      "Loss (Batch) : 0.693091924160574\n",
      "Loss (Validation) : 0.6930878864899847\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 7\n",
      "Loss (Batch) : 0.6930833717245879\n",
      "Loss (Validation) : 0.6930793478924295\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 8\n",
      "Loss (Batch) : 0.6930748179049122\n",
      "Loss (Validation) : 0.6930708106951741\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 9\n",
      "Loss (Batch) : 0.6930662616488958\n",
      "Loss (Validation) : 0.6930622754683917\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 10\n",
      "Loss (Batch) : 0.6930577009673563\n",
      "Loss (Validation) : 0.6930537655088679\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 11\n",
      "Loss (Batch) : 0.693049139382783\n",
      "Loss (Validation) : 0.693045248710151\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 12\n",
      "Loss (Batch) : 0.6930405582603999\n",
      "Loss (Validation) : 0.6930366631461257\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 13\n",
      "Loss (Batch) : 0.693031940471228\n",
      "Loss (Validation) : 0.693028051634023\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 14\n",
      "Loss (Batch) : 0.6930232981443519\n",
      "Loss (Validation) : 0.6930194264776082\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 15\n",
      "Loss (Batch) : 0.6930146375188511\n",
      "Loss (Validation) : 0.6930107781378738\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 16\n",
      "Loss (Batch) : 0.6930059563540582\n",
      "Loss (Validation) : 0.6930021038364809\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 17\n",
      "Loss (Batch) : 0.6929972502109233\n",
      "Loss (Validation) : 0.6929933990046401\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 18\n",
      "Loss (Batch) : 0.692988514989234\n",
      "Loss (Validation) : 0.692984673750755\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 19\n",
      "Loss (Batch) : 0.6929797413663729\n",
      "Loss (Validation) : 0.6929759297496382\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 20\n",
      "Loss (Batch) : 0.6929709265756577\n",
      "Loss (Validation) : 0.6929671340643213\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 21\n",
      "Loss (Batch) : 0.6929620656285552\n",
      "Loss (Validation) : 0.6929582813592817\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 22\n",
      "Loss (Batch) : 0.6929531500694392\n",
      "Loss (Validation) : 0.6929493642158756\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 23\n",
      "Loss (Batch) : 0.6929441711544\n",
      "Loss (Validation) : 0.6929403728338346\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 24\n",
      "Loss (Batch) : 0.6929351189588521\n",
      "Loss (Validation) : 0.6929312955718532\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 25\n",
      "Loss (Batch) : 0.6929259821663865\n",
      "Loss (Validation) : 0.6929221198298638\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 26\n",
      "Loss (Batch) : 0.692916748464911\n",
      "Loss (Validation) : 0.6929128308360805\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 27\n",
      "Loss (Batch) : 0.6929074025013657\n",
      "Loss (Validation) : 0.6929034123597276\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 28\n",
      "Loss (Batch) : 0.6928979275482688\n",
      "Loss (Validation) : 0.6928938434355398\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 29\n",
      "Loss (Batch) : 0.6928883072086541\n",
      "Loss (Validation) : 0.692884121589996\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 30\n",
      "Loss (Batch) : 0.6928785363547341\n",
      "Loss (Validation) : 0.6928742236093295\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 31\n",
      "Loss (Batch) : 0.6928685846812921\n",
      "Loss (Validation) : 0.6928641015456504\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 32\n",
      "Loss (Batch) : 0.6928584122403016\n",
      "Loss (Validation) : 0.6928537211387257\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 33\n",
      "Loss (Batch) : 0.6928479799968607\n",
      "Loss (Validation) : 0.6928430381914237\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 34\n",
      "Loss (Batch) : 0.6928372448049901\n",
      "Loss (Validation) : 0.6928320023521748\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 35\n",
      "Loss (Batch) : 0.6928261555221069\n",
      "Loss (Validation) : 0.6928205375208537\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 36\n",
      "Loss (Batch) : 0.6928146530585643\n",
      "Loss (Validation) : 0.6928085734552598\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 37\n",
      "Loss (Batch) : 0.6928026571162389\n",
      "Loss (Validation) : 0.6927960304040115\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 38\n",
      "Loss (Batch) : 0.6927900740686426\n",
      "Loss (Validation) : 0.6927827989939253\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 39\n",
      "Loss (Batch) : 0.692776789759291\n",
      "Loss (Validation) : 0.6927687449124978\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 40\n",
      "Loss (Batch) : 0.6927626634590163\n",
      "Loss (Validation) : 0.6927537007211035\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 41\n",
      "Loss (Batch) : 0.6927475198754809\n",
      "Loss (Validation) : 0.6927374572305944\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 42\n",
      "Loss (Batch) : 0.6927311387742298\n",
      "Loss (Validation) : 0.6927197511159612\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 43\n",
      "Loss (Batch) : 0.6927132450499077\n",
      "Loss (Validation) : 0.6927002622882698\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 44\n",
      "Loss (Batch) : 0.6926934882945586\n",
      "Loss (Validation) : 0.6926785509346153\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 45\n",
      "Loss (Batch) : 0.6926714041867512\n",
      "Loss (Validation) : 0.6926540592072351\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 46\n",
      "Loss (Batch) : 0.6926463931569844\n",
      "Loss (Validation) : 0.6926260565927866\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 47\n",
      "Loss (Batch) : 0.6926176668725097\n",
      "Loss (Validation) : 0.6925935612338714\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 48\n",
      "Loss (Batch) : 0.6925841735367094\n",
      "Loss (Validation) : 0.6925552927214351\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 49\n",
      "Loss (Batch) : 0.6925444958311169\n",
      "Loss (Validation) : 0.6925094937434483\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 50\n",
      "Loss (Batch) : 0.6924966990720663\n",
      "Loss (Validation) : 0.6924537489871903\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 51\n",
      "Loss (Batch) : 0.6924381130411958\n",
      "Loss (Validation) : 0.6923847043206489\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 52\n",
      "Loss (Batch) : 0.6923650105270424\n",
      "Loss (Validation) : 0.6922976478520727\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 53\n",
      "Loss (Batch) : 0.6922721437690372\n",
      "Loss (Validation) : 0.6921859128251455\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 54\n",
      "Loss (Batch) : 0.6921520978817429\n",
      "Loss (Validation) : 0.692039997148538\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 55\n",
      "Loss (Batch) : 0.6919944409026331\n",
      "Loss (Validation) : 0.6918465418895422\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 56\n",
      "Loss (Batch) : 0.6917851379052748\n",
      "Loss (Validation) : 0.6915880240375608\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 57\n",
      "Loss (Batch) : 0.6915072291714597\n",
      "Loss (Validation) : 0.6912439047418071\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 58\n",
      "Loss (Batch) : 0.691147022517033\n",
      "Loss (Validation) : 0.6908031682174222\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 59\n",
      "Loss (Batch) : 0.690720486721286\n",
      "Loss (Validation) : 0.6903096857941161\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 60\n",
      "Loss (Batch) : 0.6903583452679043\n",
      "Loss (Validation) : 0.6900005360387566\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 61\n",
      "Loss (Batch) : 0.6905392052394165\n",
      "Loss (Validation) : 0.6906530753114868\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Batch) : 0.692548295536439\n",
      "Loss (Validation) : 0.6941207553664146\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 63\n",
      "Loss (Batch) : 0.698799333329936\n",
      "Loss (Validation) : 0.7032363738735993\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 64\n",
      "Loss (Batch) : 0.7118945465408809\n",
      "Loss (Validation) : 0.7200311032655875\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 65\n",
      "Loss (Batch) : 0.7328730503091997\n",
      "Loss (Validation) : 0.744735996411954\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 66\n",
      "Loss (Batch) : 0.7624084203766205\n",
      "Loss (Validation) : 0.7795155721961377\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 67\n",
      "Loss (Batch) : 0.8086580363957032\n",
      "Loss (Validation) : 0.8435122568724226\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 68\n",
      "Loss (Batch) : 1.897381947308034\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 69\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 70\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 71\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 72\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 73\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 74\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 75\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 76\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 77\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 78\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 79\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 80\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 81\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 82\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 83\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 84\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 85\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 86\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 87\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 88\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 89\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 90\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 91\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 92\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 93\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 94\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 95\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 96\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 97\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 98\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Epoch index : 99\n",
      "Loss (Batch) : 12.73694258214762\n",
      "Loss (Validation) : 12.75054601602058\n",
      "Accuracy (Validation) : 0.5385427214512886\n",
      "Runtime of the algorithm is 72.02797823499998 seconds\n"
     ]
    }
   ],
   "source": [
    "leakly_relu_parameter = 0.001\n",
    "batch_size = 10000\n",
    "number_of_epochs = 100\n",
    "learning_rate = 0.000000001\n",
    "neuron_numbers = [7, 4]\n",
    "neural_network = neural_network(X_train.shape[1], neuron_numbers, leakly_relu_parameter)\n",
    "start_predict = timeit.default_timer()\n",
    "training_error, validation_error, validation_accuracy = neural_network.train(X_train, y_train, X_validation, y_validation, batch_size, number_of_epochs, learning_rate)\n",
    "stop_predict = timeit.default_timer()\n",
    "print('Runtime of the algorithm is', stop_predict - start_predict, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Test) : 12.668874949419497\n",
      "Accuracy (Test) : 0.5434060213642911\n"
     ]
    }
   ],
   "source": [
    "neural_network.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931430361118915,\n",
       " 0.6931345011582707,\n",
       " 0.6931259729298807,\n",
       " 0.6931174566543803,\n",
       " 0.693108950893459,\n",
       " 0.6931004431408706,\n",
       " 0.693091924160574,\n",
       " 0.6930833717245879,\n",
       " 0.6930748179049122,\n",
       " 0.6930662616488958,\n",
       " 0.6930577009673563,\n",
       " 0.693049139382783,\n",
       " 0.6930405582603999,\n",
       " 0.693031940471228,\n",
       " 0.6930232981443519,\n",
       " 0.6930146375188511,\n",
       " 0.6930059563540582,\n",
       " 0.6929972502109233,\n",
       " 0.692988514989234,\n",
       " 0.6929797413663729,\n",
       " 0.6929709265756577,\n",
       " 0.6929620656285552,\n",
       " 0.6929531500694392,\n",
       " 0.6929441711544,\n",
       " 0.6929351189588521,\n",
       " 0.6929259821663865,\n",
       " 0.692916748464911,\n",
       " 0.6929074025013657,\n",
       " 0.6928979275482688,\n",
       " 0.6928883072086541,\n",
       " 0.6928785363547341,\n",
       " 0.6928685846812921,\n",
       " 0.6928584122403016,\n",
       " 0.6928479799968607,\n",
       " 0.6928372448049901,\n",
       " 0.6928261555221069,\n",
       " 0.6928146530585643,\n",
       " 0.6928026571162389,\n",
       " 0.6927900740686426,\n",
       " 0.692776789759291,\n",
       " 0.6927626634590163,\n",
       " 0.6927475198754809,\n",
       " 0.6927311387742298,\n",
       " 0.6927132450499077,\n",
       " 0.6926934882945586,\n",
       " 0.6926714041867512,\n",
       " 0.6926463931569844,\n",
       " 0.6926176668725097,\n",
       " 0.6925841735367094,\n",
       " 0.6925444958311169,\n",
       " 0.6924966990720663,\n",
       " 0.6924381130411958,\n",
       " 0.6923650105270424,\n",
       " 0.6922721437690372,\n",
       " 0.6921520978817429,\n",
       " 0.6919944409026331,\n",
       " 0.6917851379052748,\n",
       " 0.6915072291714597,\n",
       " 0.691147022517033,\n",
       " 0.690720486721286,\n",
       " 0.6903583452679043,\n",
       " 0.6905392052394165,\n",
       " 0.692548295536439,\n",
       " 0.698799333329936,\n",
       " 0.7118945465408809,\n",
       " 0.7328730503091997,\n",
       " 0.7624084203766205,\n",
       " 0.8086580363957032,\n",
       " 1.897381947308034,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762,\n",
       " 12.73694258214762]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6931386733271366,\n",
       " 0.6931302043413846,\n",
       " 0.6931217428053259,\n",
       " 0.6931132736749201,\n",
       " 0.6931048142882514,\n",
       " 0.6930963792163491,\n",
       " 0.6930878864899847,\n",
       " 0.6930793478924295,\n",
       " 0.6930708106951741,\n",
       " 0.6930622754683917,\n",
       " 0.6930537655088679,\n",
       " 0.693045248710151,\n",
       " 0.6930366631461257,\n",
       " 0.693028051634023,\n",
       " 0.6930194264776082,\n",
       " 0.6930107781378738,\n",
       " 0.6930021038364809,\n",
       " 0.6929933990046401,\n",
       " 0.692984673750755,\n",
       " 0.6929759297496382,\n",
       " 0.6929671340643213,\n",
       " 0.6929582813592817,\n",
       " 0.6929493642158756,\n",
       " 0.6929403728338346,\n",
       " 0.6929312955718532,\n",
       " 0.6929221198298638,\n",
       " 0.6929128308360805,\n",
       " 0.6929034123597276,\n",
       " 0.6928938434355398,\n",
       " 0.692884121589996,\n",
       " 0.6928742236093295,\n",
       " 0.6928641015456504,\n",
       " 0.6928537211387257,\n",
       " 0.6928430381914237,\n",
       " 0.6928320023521748,\n",
       " 0.6928205375208537,\n",
       " 0.6928085734552598,\n",
       " 0.6927960304040115,\n",
       " 0.6927827989939253,\n",
       " 0.6927687449124978,\n",
       " 0.6927537007211035,\n",
       " 0.6927374572305944,\n",
       " 0.6927197511159612,\n",
       " 0.6927002622882698,\n",
       " 0.6926785509346153,\n",
       " 0.6926540592072351,\n",
       " 0.6926260565927866,\n",
       " 0.6925935612338714,\n",
       " 0.6925552927214351,\n",
       " 0.6925094937434483,\n",
       " 0.6924537489871903,\n",
       " 0.6923847043206489,\n",
       " 0.6922976478520727,\n",
       " 0.6921859128251455,\n",
       " 0.692039997148538,\n",
       " 0.6918465418895422,\n",
       " 0.6915880240375608,\n",
       " 0.6912439047418071,\n",
       " 0.6908031682174222,\n",
       " 0.6903096857941161,\n",
       " 0.6900005360387566,\n",
       " 0.6906530753114868,\n",
       " 0.6941207553664146,\n",
       " 0.7032363738735993,\n",
       " 0.7200311032655875,\n",
       " 0.744735996411954,\n",
       " 0.7795155721961377,\n",
       " 0.8435122568724226,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058,\n",
       " 12.75054601602058]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886,\n",
       " 0.5385427214512886]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
